import streamlit as st
from langchain_openai import ChatOpenAI 
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸°
from utils.rag_engine import build_retriever
from utils.prompts import get_prompt_by_mode
from dotenv import load_dotenv  # [ìˆ˜ì • 2] í™˜ê²½ë³€ìˆ˜ ë¡œë“œ

load_dotenv()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="TFT AI ë± ì¶”ì²œ", page_icon="ğŸ§")

# [1] ë¦¬ì†ŒìŠ¤ ìºì‹± (ìƒˆë¡œê³ ì¹¨ ë•Œë§ˆë‹¤ DB ë‹¤ì‹œ ë§Œë“¤ì§€ ì•Šë„ë¡)
@st.cache_resource
def get_retriever():
    return build_retriever()

retriever = get_retriever()

# [2] ì‚¬ì´ë“œë°” & ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
def init_session():
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "mode" not in st.session_state:
        st.session_state.mode = "general" # ê¸°ë³¸ ëª¨ë“œ
    if "sub_mode" not in st.session_state:
        st.session_state.sub_mode = None

init_session()

# [3] RAG ì²´ì¸ ì‹¤í–‰ í•¨ìˆ˜
# [ìˆ˜ì • 3] run_rag_chain í•¨ìˆ˜ ë‚´ë¶€ ëª¨ë¸ ë³€ê²½
def run_rag_chain(user_input, mode, sub_mode):
    # 1. Retriever í™•ì¸
    if retriever is None:
        yield "âš ï¸ [ì‹œìŠ¤í…œ ì˜¤ë¥˜] ë°ì´í„° íŒŒì¼ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        return

    # 2. ì—ëŸ¬ í•¸ë“¤ë§ì„ ìœ„í•œ try-except ë¸”ë¡
    try:
        # ëª¨ë¸ ì„¤ì • (GPT-4o-mini)
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        
        # í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
        prompt_template = get_prompt_by_mode(mode, sub_mode)
        
        def format_docs(docs):
            # ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
            if not docs:
                return "ê²€ìƒ‰ëœ ê´€ë ¨ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
            return "\n\n".join([d.page_content for d in docs])

        # ì²´ì¸ êµ¬ì„±
        chain = (
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
            | prompt_template
            | llm
            | StrOutputParser()
        )
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ (ì—¬ê¸°ì„œ ì—ëŸ¬ê°€ ë‚˜ë©´ exceptë¡œ ë„˜ì–´ê°)
        for chunk in chain.stream(user_input):
            yield chunk

    except Exception as e:
        # ì—ëŸ¬ ë‚´ìš©ì„ í™”ë©´ì— ì¶œë ¥
        error_message = str(e)
        
        if "401" in error_message:
            yield "ğŸš¨ [ì¸ì¦ ì˜¤ë¥˜] API Keyê°€ í‹€ë ¸ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”."
        elif "429" in error_message:
            yield "ğŸ’¸ [ê²°ì œ ì˜¤ë¥˜] OpenAI ê³„ì •ì˜ ì”ì•¡(Credit)ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ê²°ì œ ì •ë³´ë¥¼ ë“±ë¡í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."
        elif "not found" in error_message:
            yield "âŒ [ëª¨ë¸ ì˜¤ë¥˜] ëª¨ë¸ ì´ë¦„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (gpt-4o-mini ì² ì í™•ì¸)"
        else:
            yield f"âš ï¸ [ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ] \n\n{error_message}"
            
# [4] ë©”ì¸ UI êµ¬ì„±
def main():
    st.title("ğŸ§ TFT ë¡¤ì²´ ë„ìš°ë¯¸ (RAG)")
    
    # --- ëª¨ë“œ ì„ íƒ ë²„íŠ¼ ì˜ì—­ ---
    st.markdown("### ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
    col1, col2, col3 = st.columns(3)
    
    if col1.button("ğŸƒ ë± ì¶”ì²œ", use_container_width=True):
        st.session_state.mode = "deck_rec"
        st.session_state.sub_mode = "champion" # ê¸°ë³¸ê°’
        
    if col2.button("âš”ï¸ ì•„ì´í…œ ì¶”ì²œ", use_container_width=True):
        st.session_state.mode = "item_rec"
        st.session_state.sub_mode = "champion"
        
    if col3.button("ğŸ’ ì¦ê°• ì¶”ì²œ", use_container_width=True):
        st.session_state.mode = "augment_rec"
        st.session_state.sub_mode = None

    # --- ì„¸ë¶€ ìƒí™© ì„ íƒ (Radio Button) ---
    if st.session_state.mode == "deck_rec":
        st.info("í˜„ì¬ ëª¨ë“œ: **ë± ì¶”ì²œ**")
        st.session_state.sub_mode = st.radio(
            "ì–´ë–¤ ìƒí™©ì¸ê°€ìš”?",
            ["ì±”í”¼ì–¸ ê¸°ë°˜ (ì˜ ëœ¬ ê¸°ë¬¼)", "ì•„ì´í…œ ê¸°ë°˜ (ë³´ìœ  ì•„ì´í…œ)", "ìƒì§•/íŠ¹ì„± ê¸°ë°˜"],
            horizontal=True
        )
    elif st.session_state.mode == "item_rec":
        st.info("í˜„ì¬ ëª¨ë“œ: **ì•„ì´í…œ ì¶”ì²œ**")
        st.session_state.sub_mode = st.radio(
            "ì–´ë–¤ ìƒí™©ì¸ê°€ìš”?",
            ["íŠ¹ì • ì±”í”¼ì–¸ì—ê²Œ ì¤„ ì•„ì´í…œ", "í˜„ì¬ ë±ì— ë‚¨ëŠ” ì•„ì´í…œ ì²˜ë¦¬"],
            horizontal=True
        )
    elif st.session_state.mode == "augment_rec":
        st.info("í˜„ì¬ ëª¨ë“œ: **ì¦ê°•ì²´ ì¶”ì²œ**")

    st.divider()

    # --- ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ---
    # 1. ì´ì „ ëŒ€í™” ì¶œë ¥
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # 2. ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
        # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # AI ë‹µë³€ ìƒì„±
        with st.chat_message("assistant"):
            response_placeholder = st.empty()
            full_response = ""
            
            # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë‹µë³€ ë°›ì•„ì˜¤ê¸°
            mode = st.session_state.mode
            sub = st.session_state.sub_mode
            
            # RAG ì‹¤í–‰
            stream = run_rag_chain(prompt, mode, sub)
            
            for chunk in stream:
                full_response += chunk
                response_placeholder.markdown(full_response + "â–Œ")
            
            response_placeholder.markdown(full_response)
        
        # ëŒ€í™” ì €ì¥
        st.session_state.messages.append({"role": "assistant", "content": full_response})

if __name__ == '__main__':
    main()