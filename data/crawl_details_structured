import json
import time
import os
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from tqdm import tqdm

# ==========================================
# [ì„¤ì •] íŒŒì¼ ê²½ë¡œ
# ==========================================
INPUT_FILE = "data/lolchess_meta_list.json"  # 1ë‹¨ê³„ì—ì„œ ë§Œë“  JSON íŒŒì¼
OUTPUT_FILE = "data/lolchess_guide_structured.json" # ìµœì¢… ê²°ê³¼ íŒŒì¼

# ==========================================
# [í•µì‹¬] HTML êµ¬ì¡° íŒŒì„œ (êµ¬ì¡°ì  ë°ì´í„° ì¶”ì¶œ)
# ==========================================
def parse_html_structure(container_html):
    """
    HTML ë©ì–´ë¦¬ ì•ˆì—ì„œ H1(ì œëª©)ê³¼ P(ë‚´ìš©) ê´€ê³„ë¥¼ êµ¬ì¡°í™”í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜
    ì˜ˆ: {"ê°œìš”": ["ë‚œì´ë„ ë³´í†µ", "ë¹Œë“œì—… ì¶”ì²œ..."], "ì•„ì´í…œ": [...]}
    """
    if not container_html:
        return {}

    soup = BeautifulSoup(str(container_html), 'html.parser')
    structured_data = {}
    current_key = "General" # H1ì´ ì—†ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ê¸°ë³¸ í‚¤

    # challenger-comment í´ë˜ìŠ¤ ë‚´ë¶€ì˜ ëª¨ë“  ìì‹ íƒœê·¸ë¥¼ ìˆœì„œëŒ€ë¡œ ìˆœíšŒ
    content_div = soup.find("div", class_="challenger-comment")
    if not content_div:
        return {"content": soup.get_text(strip=True)}

    for element in content_div.children:
        if element.name in ['h1', 'h2', 'h3', 'strong']: # ì†Œì œëª© ì¸ì‹
            current_key = element.get_text(strip=True)
            if current_key not in structured_data:
                structured_data[current_key] = []
        elif element.name == 'p' or element.name == 'div': # ë‚´ìš© ì¸ì‹
            text = element.get_text(separator=" ", strip=True)
            if text:
                if current_key not in structured_data:
                    structured_data[current_key] = []
                structured_data[current_key].append(text)
    
    return structured_data

def crawl_details():
    # 1. ë©”íƒ€ ë°ì´í„° ë¡œë“œ
    if not os.path.exists(INPUT_FILE):
        print(f"âŒ {INPUT_FILE} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. fetch_meta_data.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        meta_list = json.load(f)

    print(f">>> ì´ {len(meta_list)}ê°œì˜ ë± ìƒì„¸ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")

    # 2. ë¸Œë¼ìš°ì € ì„¤ì •
    chrome_options = Options()
    chrome_options.add_argument("--headless") 
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    # ì´ë¯¸ì§€ ë¡œë”© ë¹„í™œì„±í™” (ì†ë„ í–¥ìƒ)
    chrome_options.add_argument("--blink-settings=imagesEnabled=false") 
    
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
    
    final_results = []

    # 3. í¬ë¡¤ë§ ë£¨í”„
    for idx, deck in enumerate(tqdm(meta_list)):
        url = deck.get('url')
        if not url: continue

        try:
            driver.get(url)
            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° (ê°€ì´ë“œ ë°•ìŠ¤ê°€ ëœ° ë•Œê¹Œì§€)
            try:
                WebDriverWait(driver, 5).until(
                    EC.presence_of_element_located((By.CLASS_NAME, "challenger-comment"))
                )
            except:
                pass # ê°€ì´ë“œê°€ ì—†ëŠ” ë±ì¼ ìˆ˜ë„ ìˆìŒ

            soup = BeautifulSoup(driver.page_source, "html.parser")
            
            # --- ë°ì´í„° ì¶”ì¶œ ì‹œì‘ ---
            
            # 1. ê°€ì´ë“œ ë°•ìŠ¤ë“¤ ì°¾ê¸° (ê³µëµ & ì¦ê°•ì²´)
            # css-1s5hngw í´ë˜ìŠ¤ê°€ ê°€ì´ë“œ ë°•ìŠ¤ ì»¨í…Œì´ë„ˆ (ë³€ë™ ê°€ëŠ¥ì„± ìˆìœ¼ë‹ˆ ì£¼ì˜)
            # êµ¬ì¡°ì ìœ¼ë¡œ 'h2' íƒœê·¸ê°€ ì§ê³„ ìì‹ìœ¼ë¡œ ìˆëŠ” divë¥¼ ì°¾ëŠ” ê²ƒì´ ì•ˆì „
            guide_boxes = soup.find_all("div", class_=lambda x: x and "css-1s5hngw" in x)
            
            guide_structured = {}
            augments_structured = {}
            detail_deck_name = ""

            for box in guide_boxes:
                title_tag = box.find("h2")
                if not title_tag: continue
                title_text = title_tag.get_text(strip=True)

                if "ì¦ê°•ì²´" in title_text:
                    # [ì¦ê°•ì²´ íŒŒì‹±]
                    augments_structured = parse_html_structure(box)
                else:
                    # [ê³µëµ íŒŒì‹±]
                    # ì²« ë²ˆì§¸ë¡œ ë°œê²¬ë˜ëŠ” ì¼ë°˜ ê°€ì´ë“œ ë°•ìŠ¤ì˜ ì œëª©ì„ 'ìƒì„¸ ë± ì´ë¦„'ìœ¼ë¡œ ê°„ì£¼
                    if not detail_deck_name:
                        detail_deck_name = title_text
                    
                    # ê³µëµ ë‚´ìš© êµ¬ì¡°í™” (h1 -> p)
                    section_data = parse_html_structure(box)
                    guide_structured.update(section_data)

            # 2. ë°°ì¹˜ ì •ë³´ (ì´ì „ ë¡œì§ ìœ ì§€)
            positioning = []
            board = soup.find("div", class_=lambda x: x and "css-y6vj5x" in x)
            if board:
                slots = board.find_all("div", class_=lambda x: x and "Slot" in x)
                for i, slot in enumerate(slots):
                    name_div = slot.select_one(".css-16jrvsm") or slot.find("div", class_=lambda x: x and "ed21b2i3" in x)
                    if name_div:
                        champ_name = name_div.get_text(strip=True)
                        stars = len(slot.find_all("div", class_=lambda x: x and "bg-black" in x))
                        
                        items = []
                        for img in slot.find_all("img", src=True):
                            if "items" in img['src']:
                                items.append(img['src'].split("/")[-1].split("_")[0].split(".")[0])
                        
                        row = (i // 7) + 1
                        col = (i % 7) + 1
                        positioning.append({
                            "champion": champ_name,
                            "star": stars if stars > 0 else 1,
                            "items": items,
                            "grid": [row, col] # ì¢Œí‘œ (1~4ì—´)
                        })

            # 3. ìµœì¢… ë°ì´í„° ë³‘í•©
            deck_full_data = {
                "meta_info": deck,             # 1ë‹¨ê³„ ìˆ˜ì§‘ ì •ë³´ (hot, gold ë“±)
                "detail_deck_name": detail_deck_name, # ìƒì„¸ í˜ì´ì§€ ë‚´ë¶€ ë± ì´ë¦„ (ì˜ˆ: ìŠˆë¦¬ë§ˆ ì•„ì§€ë¥´ ë±)
                "guide": guide_structured,     # êµ¬ì¡°í™”ëœ ê³µëµ (ê°œìš”: [...], ì•„ì´í…œ: [...])
                "augments": augments_structured, # êµ¬ì¡°í™”ëœ ì¦ê°•ì²´ (ì‹¤ë²„: [...], ê³¨ë“œ: [...])
                "positioning": positioning     # ë°°ì¹˜ ì •ë³´
            }
            
            final_results.append(deck_full_data)
            # time.sleep(0.5) # ì„œë²„ ë¶€í•˜ ë°©ì§€ìš© ì§§ì€ ëŒ€ê¸°

        except Exception as e:
            print(f"Error processing {url}: {e}")
            # ì—ëŸ¬ ë‚˜ë„ ë©ˆì¶”ì§€ ì•Šê³  ë‹¤ìŒ ë±ìœ¼ë¡œ ì§„í–‰
            continue

    driver.quit()

    # 4. ê²°ê³¼ ì €ì¥
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(final_results, f, indent=4, ensure_ascii=False)
    
    print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ! {len(final_results)}ê°œì˜ ìƒì„¸ ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"ğŸ“‚ íŒŒì¼ ìœ„ì¹˜: {OUTPUT_FILE}")

if __name__ == "__main__":
    crawl_details()